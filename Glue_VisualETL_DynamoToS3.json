{
	"jobConfig": {
		"name": "Glue_VisualETL_DynamoToS3",
		"description": "",
		"role": "arn:aws:iam::557690608573:role/sprint11",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Glue_VisualETL_DynamoToS3.py",
		"scriptLocation": "s3://aws-glue-assets-557690608573-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-05T00:33:40.221Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-557690608573-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-557690608573-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"logging": false
	},
	"dag": {
		"node-1751675457490": {
			"nodeId": "node-1751675457490",
			"dataPreview": false,
			"previewAmount": 0,
			"inputs": [
				"node-1751675450738"
			],
			"name": "Amazon S3",
			"generatedNodeName": "AmazonS3_node1751675457490",
			"classification": "DataSink",
			"type": "S3",
			"streamingBatchInterval": 100,
			"format": "glueparquet",
			"compression": "none",
			"numberTargetPartitions": "0",
			"path": "s3://my-output-bucket20250704/dynamodb-export/",
			"partitionKeys": [],
			"schemaChangePolicy": {
				"enableUpdateCatalog": false,
				"updateBehavior": null,
				"database": null,
				"table": null
			},
			"updateCatalogOptions": "none",
			"autoDataQuality": {
				"isEnabled": true,
				"evaluationContext": "EvaluateDataQuality_node1751675408596"
			},
			"calculatedType": "",
			"codeGenVersion": 2
		},
		"node-1751675450738": {
			"nodeId": "node-1751675450738",
			"dataPreview": false,
			"previewAmount": 0,
			"inputs": [],
			"name": "Amazon DynamoDB",
			"generatedNodeName": "AmazonDynamoDB_node1751675450738",
			"classification": "DataSource",
			"type": "DBB",
			"isCatalog": true,
			"database": "dynamodb_data_sprint11",
			"table": "inquirytable",
			"pitrEnabled": false,
			"additionalOptions": null,
			"calculatedType": "",
			"ddbConnectorType": "catalog",
			"codeGenVersion": 2
		}
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom awsgluedq.transforms import EvaluateDataQuality\r\nfrom awsglue.dynamicframe import DynamicFrame  # ← これが必要\r\n\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Default ruleset used by all target nodes with data quality enabled\r\nDEFAULT_DATA_QUALITY_RULESET = \"\"\"\r\n    Rules = [\r\n        ColumnCount > 0\r\n    ]\r\n\"\"\"\r\n\r\n# DynamoDB から読み込み\r\nAmazonDynamoDB_node1751675450738 = glueContext.create_dynamic_frame.from_catalog(\r\n    database=\"dynamodb_data_sprint11\",\r\n    table_name=\"inquirytable\",\r\n    transformation_ctx=\"AmazonDynamoDB_node1751675450738\"\r\n)\r\n\r\n# データ品質チェック（変更なし）\r\nEvaluateDataQuality().process_rows(\r\n    frame=AmazonDynamoDB_node1751675450738,\r\n    ruleset=DEFAULT_DATA_QUALITY_RULESET,\r\n    publishing_options={\r\n        \"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1751675408596\",\r\n        \"enableDataQualityResultsPublishing\": True\r\n    },\r\n    additional_options={\r\n        \"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\",\r\n        \"observations.scope\": \"ALL\"\r\n    }\r\n)\r\n\r\n# ↓↓↓ ここから追加：1ファイルにまとめる処理 ↓↓↓\r\n\r\n# DynamicFrame → DataFrame に変換\r\ndf = AmazonDynamoDB_node1751675450738.toDF()\r\n\r\n# 1ファイルにまとめる\r\ndf_single = df.coalesce(1)\r\n\r\n# DataFrame → DynamicFrame に戻す\r\ndyf_single = DynamicFrame.fromDF(df_single, glueContext, \"dyf_single\")\r\n\r\n# S3 に書き込み\r\nAmazonS3_node1751675457490 = glueContext.write_dynamic_frame.from_options(\r\n    frame=dyf_single,\r\n    connection_type=\"s3\",\r\n    format=\"glueparquet\",  # parquet形式（glueparquetは内部的には同じ）\r\n    connection_options={\"path\": \"s3://my-output-bucket20250704/dynamodb-export/\", \"partitionKeys\": []},\r\n    transformation_ctx=\"AmazonS3_node1751675457490\"\r\n)\r\n\r\n# ↑↑↑ ここまで追加 ↑↑↑\r\n\r\njob.commit()\r\n"
}